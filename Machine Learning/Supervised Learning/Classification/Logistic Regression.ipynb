{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9619eb5-7c6e-4a6b-8971-a9e379778b30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#  Logistic Regression\n",
    "---\n",
    "## Intuition\n",
    "\n",
    "> **Logistic Regression = Linear Regression + Sigmoid + Log Loss**\n",
    "---\n",
    "## Equation\n",
    "$$\n",
    "\\hat{y} = \\frac{1}{1 + e^{-(XW + b)}}\n",
    "$$\n",
    "----\n",
    "## Sigmoid Function\n",
    "\n",
    "![Sigmoid Function](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "##  Problem Setup\n",
    "\n",
    "We are given:\n",
    "\n",
    "- **X** → Input features (shape: m X n)\n",
    "- **y** → Target labels (0 or 1)\n",
    "- **W** → Weights\n",
    "- **b** → Bias\n",
    "\n",
    "### Goal:\n",
    "Predict the probability:\n",
    "\n",
    "$$\n",
    "P(y = 1 \\mid X)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##  Linear Model (Same as Linear Regression)\n",
    "\n",
    "$$\n",
    "z = XW + b\n",
    "$$\n",
    "\n",
    "z can take any value from **−∞ to +∞**\n",
    "\n",
    "---\n",
    "\n",
    "##  Sigmoid Function (Key Difference)\n",
    "\n",
    "To convert `z` into probability, we apply **Sigmoid**:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(z)\n",
    "$$\n",
    "\n",
    "### Properties:\n",
    "- Output range → **(0, 1)**\n",
    "- Large positive `z` → probability ≈ **1**\n",
    "- Large negative `z` → probability ≈ **0**\n",
    "\n",
    "---\n",
    "\n",
    "##  Decision Rule (Classification)\n",
    "\n",
    "$$\n",
    "\\text{Prediction} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } \\hat{y} \\ge 0.5 \\\\\n",
    "0 & \\text{if } \\hat{y} < 0.5\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Threshold `0.5` is default (can be tuned)\n",
    "\n",
    "---\n",
    "\n",
    "##  Why NOT Mean Squared Error (MSE)?\n",
    "\n",
    "MSE leads to:\n",
    "- Slow convergence\n",
    "- Non-convex cost with sigmoid\n",
    "\n",
    "Logistic Regression uses **Log Loss**\n",
    "\n",
    "---\n",
    "\n",
    "## Loss Function (Binary Cross-Entropy)\n",
    "\n",
    "For a **single data point**:\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) =\n",
    "-\\left[\n",
    "y \\log(\\hat{y}) +\n",
    "(1 - y)\\log(1 - \\hat{y})\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Penalizes **confident wrong predictions heavily**\n",
    "\n",
    "---\n",
    "\n",
    "## Cost Function (Entire Dataset)\n",
    "\n",
    "$$\n",
    "J(W, b) =\n",
    "-\\frac{1}{m}\n",
    "\\sum_{i=1}^{m}\n",
    "\\left[\n",
    "y^{(i)}\\log(\\hat{y}^{(i)}) +\n",
    "(1 - y^{(i)})\\log(1 - \\hat{y}^{(i)})\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "This function is **convex** → global minimum guaranteed\n",
    "\n",
    "---\n",
    "\n",
    "##  Gradient Descent (Training)\n",
    "\n",
    "### Gradients:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W} =\n",
    "\\frac{1}{m} X^T (\\hat{y} - y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} =\n",
    "\\frac{1}{m} \\sum (\\hat{y} - y)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Parameter Update Rules\n",
    "\n",
    "$$\n",
    "W := W - \\alpha \\frac{\\partial J}{\\partial W}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "-  $$ \n",
    "\\alpha \\ = learning rate\n",
    "$$\n",
    "- Training runs for multiple **epochs**\n",
    "\n",
    "---\n",
    "\n",
    "## Complete Training Flow\n",
    "\n",
    "1. Initialize W, b\n",
    "2. Repeat for epochs:\n",
    "3. z = XW + b\n",
    "4. y_hat = sigmoid(z)\n",
    "5. Compute log loss\n",
    "6. Compute gradients\n",
    "7. Update W, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "022f194f-259e-4805-b0a5-1e7f085cc365",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fdbce52-2e66-4ece-8428-4fe06507a81b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_features: int) -> (np.ndarray,float): \n",
    "    \"\"\"\n",
    "    Initialize parameters for Logistic Regression.\n",
    "\n",
    "    Parameters:\n",
    "    n_features (int): Number of input features (columns in X)\n",
    "\n",
    "    Returns:\n",
    "    W (numpy.ndarray): Weight vector of shape (n_features, 1)\n",
    "    b (float): Bias term initialized to 0.0\n",
    "    \"\"\"\n",
    "    # Initialize weight vector (one weight per feature)\n",
    "    # Shape: (n_features, 1)\n",
    "    W = np.random.rand(n_features, 1)\n",
    "\n",
    "    # Initialize bias term (scalar)\n",
    "    b = 0.0\n",
    "\n",
    "    return W, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "976f8539-59f5-44fd-83f1-2a32a5a02c94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def linear_forward(X: np.ndarray, W: np.ndarray, b: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the linear forward step for Logistic Regression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Input feature matrix of shape (m, n),\n",
    "        where m is the number of samples and n is the number of features.\n",
    "    W : np.ndarray\n",
    "        Weight vector of shape (n, 1).\n",
    "    b : float\n",
    "        Bias term (scalar).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Linear output Z of shape (m, 1).\n",
    "    \"\"\"\n",
    "    # Validate dimensions\n",
    "    if X.ndim != 2:\n",
    "        raise ValueError(\"X must be a 2D array of shape (m, n)\")\n",
    "    if W.ndim != 2 or W.shape[1] != 1:\n",
    "        raise ValueError(\"W must be a 2D column vector of shape (n, 1)\")\n",
    "    if X.shape[1] != W.shape[0]:\n",
    "        raise ValueError(\"Number of features in X must match size of W\")\n",
    "\n",
    "    Z = X.dot(W) + b\n",
    "    return Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e9e8ddf-78c5-4acf-9c75-017d7b73a21a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the sigmoid activation function in a numerically stable way.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Z : np.ndarray\n",
    "        Linear input (logits).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Sigmoid output with values in the range (0, 1).\n",
    "    \"\"\"\n",
    "    # Clip values to avoid overflow in exp\n",
    "    #We clip the logits before applying the exponential function to avoid numerical overflow and ensure stable computation of the sigmoid and loss.\n",
    "    Z_clipped = np.clip(Z, -500, 500)\n",
    "\n",
    "    mu = 1.0 / (1.0 + np.exp(-Z_clipped))\n",
    "    return mu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e07cc41-00c1-49d2-abae-36f048b136fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X: np.ndarray, W: np.ndarray, b: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform forward propagation for Logistic Regression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Input feature matrix of shape (m, n).\n",
    "    W : np.ndarray\n",
    "        Weight vector of shape (n, 1).\n",
    "    b : float\n",
    "        Bias term (scalar).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Predicted probabilities y_hat of shape (m, 1).\n",
    "    \"\"\"\n",
    "    Z = linear_forward(X, W, b)\n",
    "    y_hat = sigmoid(Z)\n",
    "\n",
    "    return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ff2eb44-f4a3-431b-9827-f3d7f8bc0734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def compute_loss(y: np.ndarray, y_hat: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute Binary Cross-Entropy (Log Loss) for Logistic Regression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.ndarray\n",
    "        True labels of shape (m, 1).\n",
    "    y_hat : np.ndarray\n",
    "        Predicted probabilities of shape (m, 1).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Scalar loss value.\n",
    "    \"\"\"\n",
    "    if y.shape != y_hat.shape:\n",
    "        raise ValueError(\"y and y_hat must have the same shape\")\n",
    "\n",
    "    m = y.shape[0]\n",
    "\n",
    "    # Numerical stability: avoid log(0)\n",
    "    epsilon = 1e-15\n",
    "    y_hat_clipped = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "\n",
    "    loss = - (1 / m) * np.sum(\n",
    "        y * np.log(y_hat_clipped) +\n",
    "        (1 - y) * np.log(1 - y_hat_clipped)\n",
    "    )\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52177ef3-af4c-4563-932f-73637f3cc02a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def compute_gradients(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    y_hat: np.ndarray\n",
    ") -> tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Compute gradients of the loss with respect to weights and bias.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Input feature matrix of shape (m, n).\n",
    "    y : np.ndarray\n",
    "        True labels of shape (m, 1).\n",
    "    y_hat : np.ndarray\n",
    "        Predicted probabilities of shape (m, 1).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[np.ndarray, float]\n",
    "        dW : Gradient w.r.t weights of shape (n, 1)\n",
    "        db : Gradient w.r.t bias (scalar)\n",
    "    \"\"\"\n",
    "    if y.shape != y_hat.shape:\n",
    "        raise ValueError(\"y and y_hat must have the same shape\")\n",
    "\n",
    "    m = y.shape[0]\n",
    "\n",
    "    error = y_hat - y  # (m, 1)\n",
    "\n",
    "    dW = (1 / m) * X.T.dot(error)   # (n, 1)\n",
    "    db = (1 / m) * np.sum(error)\n",
    "\n",
    "    return dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dcb7713-0764-4b34-bba6-55f1515114eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def update_parameters(\n",
    "    W: np.ndarray,\n",
    "    b: float,\n",
    "    dW: np.ndarray,\n",
    "    db: float,\n",
    "    learning_rate: float\n",
    ") -> tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Update weights and bias using Gradient Descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    W : np.ndarray\n",
    "        Current weight vector of shape (n, 1).\n",
    "    b : float\n",
    "        Current bias term.\n",
    "    dW : np.ndarray\n",
    "        Gradient w.r.t weights of shape (n, 1).\n",
    "    db : float\n",
    "        Gradient w.r.t bias.\n",
    "    learning_rate : float\n",
    "        Learning rate (alpha).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[np.ndarray, float]\n",
    "        Updated weights and bias.\n",
    "    \"\"\"\n",
    "    if learning_rate <= 0:\n",
    "        raise ValueError(\"learning_rate must be positive\")\n",
    "\n",
    "    W = W - learning_rate * dW\n",
    "    b = b - learning_rate * db\n",
    "\n",
    "    return W, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eae3bb5-d475-4c21-80c2-58b9cd89627e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    X_df: pd.DataFrame,\n",
    "    y_df: pd.DataFrame,\n",
    "    epochs: int,\n",
    "    learning_rate: float\n",
    ") -> tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Train Logistic Regression model using Gradient Descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Input feature matrix of shape (m, n).\n",
    "    y : np.ndarray\n",
    "        True labels of shape (m, 1).\n",
    "    epochs : int\n",
    "        Number of training iterations.\n",
    "    learning_rate : float\n",
    "        Learning rate for Gradient Descent.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[np.ndarray, float]\n",
    "        Trained weights and bias.\n",
    "    \"\"\"\n",
    "\n",
    "    X = X_df.values\n",
    "    y = y_df.values\n",
    "\n",
    "    if X.ndim != 2:\n",
    "        raise ValueError(\"X must be a 2D array\")\n",
    "    if y.ndim != 2 or y.shape[1] != 1:\n",
    "        raise ValueError(\"y must be a column vector of shape (m, 1)\")\n",
    "\n",
    "    n_features = X.shape[1]\n",
    "    W, b = initialize_parameters(n_features)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward propagation\n",
    "        y_hat = forward_propagation(X, W, b)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = compute_loss(y, y_hat)\n",
    "\n",
    "        # Compute gradients\n",
    "        dW, db = compute_gradients(X, y, y_hat)\n",
    "\n",
    "        # Update parameters\n",
    "        W, b = update_parameters(W, b, dW, db, learning_rate)\n",
    "\n",
    "        # Optional: monitor training\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "\n",
    "    return W, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5bb9d28-75e8-4944-aba0-35218e1739e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict(\n",
    "    X_df: pd.DataFrame,\n",
    "    W: np.ndarray,\n",
    "    b: float,\n",
    "    threshold: float = 0.5\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predict class labels using trained Logistic Regression parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Input feature matrix of shape (m, n).\n",
    "    W : np.ndarray\n",
    "        Trained weight vector of shape (n, 1).\n",
    "    b : float\n",
    "        Trained bias term.\n",
    "    threshold : float, optional\n",
    "        Decision threshold for classification (default is 0.5).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Predicted class labels of shape (m, 1).\n",
    "    \"\"\"\n",
    "    if not isinstance(X_df, pd.DataFrame):\n",
    "        raise TypeError(\"X_df must be a pandas DataFrame\")\n",
    "\n",
    "    X = X_df.values\n",
    "\n",
    "    if not 0 < threshold < 1:\n",
    "        raise ValueError(\"threshold must be between 0 and 1\")\n",
    "\n",
    "    # Get predicted probabilities\n",
    "    y_hat = forward_propagation(X, W, b)\n",
    "\n",
    "    # Apply threshold\n",
    "    y_pred = (y_hat >= threshold).astype(int)\n",
    "\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35e93a64-749a-4962-9fa1-3ba1af254489",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27ba8266-8bdf-4d2f-a3c2-0fbe48b9fd99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"age\": [22, 25, 47, 52],\n",
    "    \"salary\": [20000, 25000, 80000, 110000],\n",
    "    \"label\": [0, 0, 1, 1]\n",
    "})\n",
    "\n",
    "X_df = df[[\"age\", \"salary\"]]\n",
    "y_df = df[[\"label\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fabdaf2-e957-4fad-98ce-4e49333449cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "W, b = train(\n",
    "    X_df,\n",
    "    y_df,\n",
    "    epochs=1000,\n",
    "    learning_rate=0.01\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad6bda4e-3b7c-41a0-b392-ddd7f71429cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions = predict(X_df, W, b)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8154bdac-4ad8-4452-a84f-d9a9f57fb917",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "acc = accuracy(y, y_pred)\n",
    "print(\"Accuracy:\", acc)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Logistic Regression",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
